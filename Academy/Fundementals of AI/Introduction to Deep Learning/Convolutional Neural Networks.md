`Convolutional Neural Networks` (`CNNs`) are specialized neural networks designed for processing grid-like data, such as images.

A typical CNN consists of three main types of layers:

- `Convolutional Layers:` 
	- **Convolutional layers = feature detectors.**
	- They use small filters that slide across the input (like scanning with a magnifying glass).
	- Each filter detects a specific pattern (edges, corners, textures, shapes).
	- The result is a **feature map** showing _where_ those patterns appear.
- `Pooling Layers:` These layers reduce the dimensionality of the feature maps, making the network less computationally expensive and less susceptible to overfitting. They operate on each feature map independently, downsampling it by taking the maximum or average value within a small window. Common types of pooling include `max pooling` and `average pooling`.
- `Fully Connected Layers:` These layers are similar to those in `MLPs`. They connect every neuron in one layer to every neuron in the next layer. These layers are typically used towards the network's end to perform high-level reasoning and make predictions based on the extracted features.

## Feature Maps and Hierarchical Feature Learning
![](Pasted%20image%2020250821091445.png)
In a `CNN`, `feature maps` are generated by the convolutional layers. Each convolutional filter produces a corresponding feature map, highlighting the locations and strength of specific visual patterns within the input image. For example, one filter might detect edges, another corners, and another texture.

This learning process is hierarchical:
- `Initial Layers:` These layers tend to learn simple, low-level features like edges and blobs. For example, a convolutional layer might detect vertical or horizontal edges in an image.
- `Intermediate Layers:` As the network progresses, subsequent layers combine these basic features to detect more complex patterns. For instance, one intermediate layer might identify corners by combining edge detections from earlier layers.
- `Deeper Layers:` These layers learn high-level features such as shapes and object parts. For example, a deep convolutional layer might recognize wheels, windows, or entire cars in an image recognition task.
![](Pasted%20image%2020250821091626.png)
![](Pasted%20image%2020250821091635.png)
![](Pasted%20image%2020250821091642.png)
## Image Recognition
![](Pasted%20image%2020250821091711.png)
To illustrate this process, consider an image recognition task where a `CNN` is trained to classify images of different animals:

1. `Input Layer:` The input is a raw image, typically represented as a 3D tensor (height, width, channels).
2. `Convolutional Layers:`
    - `Layer 1:` Detects low-level features like edges and simple textures.
    - `Layer 2:` Combines these features to detect more complex patterns, such as corners and curves.
    - `Layer 3:` Recognizes higher-level structures like shapes and object parts.
3. `Pooling Layers:`
    - Reduce the spatial dimensions of the feature maps, making the network less computationally expensive and more robust to small translations in the input image.
4. `Fully Connected Layers:`
    - Flatten the output from the final pooling layer.
    - Perform high-level reasoning and make predictions based on the extracted features, such as classifying the image as a cat, dog, or bird.

## Data Assumptions for a CNN
### Grid-Like Data Structure
CNNs are inherently designed to work with data structured as grids. This grid-like organization is fundamental to how CNNs process information. Common examples include:

- `Images:` Represented as 2D grids, where each grid cell holds a pixel value. The dimensions typically include height, width, and channels (e.g., red, green, blue).
- `Videos:` Represented as 3D grids, extending the image concept by adding a time dimension. This results in a height, width, time, and channel structure.
### Spatial Hierarchy of Features
CNNs operate under the assumption that features within the data are organized hierarchically. This means that:

- `Lower-level features` like edges, corners, or textures are simple and localized. They are typically captured in the network's early layers.
- `Higher-level features` are more complex and abstract, built upon these lower-level features. They represent larger patterns, shapes, or even entire objects and are detected in the deeper layers of the network.
### Feature Locality
CNNs exploit the principle of feature locality, which assumes that relevant relationships between data points are primarily confined to local neighborhoods. For instance:

- In images, neighboring pixels are more likely to be correlated and form meaningful patterns than pixels far apart.
- Convolutional filters, the core building blocks of CNNs, are designed to focus on small local regions of the input (called receptive fields). This allows the network to capture these local dependencies efficiently.
### Feature Stationarity
- This means that a feature, such as a vertical edge, should be recognized as the same feature, whether on the image's left, right, or center.
- CNNs achieve this through weight sharing in convolutional layers. The same filter is applied across all positions in the input, enabling the network to detect the same feature anywhere in the data.
### Sufficient Data and Normalization
Effective training of CNNs relies on two practical considerations:

- `Sufficient data:` CNNs, like most deep learning models, are data-hungry. They require large, labeled datasets to learn complex patterns and generalize to unseen data. Insufficient data can lead to overfitting, where the model performs well on training data but poorly on new data.
- `Normalized input:` Input data should be normalized to a standard range (e.g., scaling pixel values to between 0 and 1, or -1 and 1). This ensures stable and efficient training by preventing large variations in input values from disrupting the learning process.
