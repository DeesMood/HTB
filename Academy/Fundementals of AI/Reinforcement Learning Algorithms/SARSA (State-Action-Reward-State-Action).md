![](Pasted%20image%2020250821084212.png)
`SARSA` is a model-free `reinforcement learning` algorithm that learns an optimal policy through direct environmental interaction.

Unlike `Q-learning`, which updates its `Q-values` based on the maximum `Q-value` of the next state, `SARSA` updates its `Q-values` based on the `Q-value` of the next state and the actual action taken in that state.

This key difference makes `SARSA` an `on-policy` algorithm, meaning it learns the value of the policy it is currently following. `Q-learning` is `off-policy`, learning the value of the optimal policy independent of the current policy.

- **SARSA (On-policy):**  
    Learns values **based on the actions it actually takes** while following its current policy (including exploration).
    
- **Q-learning (Off-policy):**  
    Learns values **as if it always followed the best possible policy**, even if in reality it’s exploring.

![](Pasted%20image%2020250821084422.png)
Imagine a robot learning to navigate a room with obstacles. `SARSA` guides the robot to learn a safe path by considering the immediate reward of an action and the consequences of the next action taken in the new state.

The SARSA algorithm follows these steps:

1. `Initialization:` Initialize the `Q-table` with arbitrary values (usually 0) for each state-action pair. This table will store the estimated `Q-values` for actions in different states.
2. `Choose an Action:` In the current state `s`, select an action `a` to execute. This selection is typically based on an exploration-exploitation strategy, such as `epsilon-greedy`, balancing exploring new actions with exploiting actions known to yield high rewards.
3. `Take Action and Observe:` Execute the chosen action `a` in the environment and observe the next state `s'` and the reward `r` received. This step involves interacting with the environment and gathering feedback on the consequences of the action.
4. `Choose Next Action:` In the next state `s'`, select the next action `a'` based on the current policy (e.g., `epsilon-greedy`). This step is crucial for `SARSA`'s on-policy nature, considering the actual action taken in the next state, not just the theoretically optimal action.
5. `Update Q-value:` Update the `Q-value` for the state-action pair (`s`, `a`).
6. `Update State and Action:` Update the current state and action to the next state and action: `s = s'`, `a = a'`. This prepares the algorithm for the next iteration.
7. `Iteration:` Repeat steps 2-6 until the `Q-values` converge or a maximum number of iterations is reached. This iterative process allows the agent to learn and refine its policy continuously.
## On-Policy Learning
![](Pasted%20image%2020250821084508.png)
In reinforcement learning, the learning process can be categorized into two main approaches: `on-policy` and `off-policy` learning. This distinction stems from how algorithms update their estimates of action values, which are crucial for determining the optimal policy.

- `On-policy learning:` In on-policy learning, the algorithm learns the value of its current policy. This means that the updates to the estimated action values are based on the actions taken and the rewards received while executing the current policy, including any exploratory actions.
- `Off-policy learning:` In contrast, off-policy learning allows the algorithm to learn about an optimal policy independently of the policy being followed for action selection. This means the algorithm can learn from data generated by a different policy, which can benefit exploration and learning from historical data.

![](Pasted%20image%2020250821084544.png)

## Exploration-Exploitation Strategies in SARSA
### Epsilon-Greedy
![](Pasted%20image%2020250821084714.png)
### Softmax
![](Pasted%20image%2020250821084731.png)
The `softmax` strategy assigns probabilities to actions based on their `Q-values`, with higher `Q-values` leading to higher probabilities.
### Convergence and Parameter Tuning
Convergence in `Reinforcement Learning` means the algorithm reaches a stable solution where the `Q-values` no longer change significantly with further training. This indicates that the agent has learned a policy that effectively maximizes rewards.

Two crucial parameters that influence the learning process are the `learning rate (α)` and the `discount factor (γ)`:

- `Learning Rate (α)`: Controls the extent of `Q-value` updates in each iteration. A high `α` leads to faster updates but can cause instability, while a low `α` ensures more stable convergence but slows down learning.
- `Discount Factor (γ)`: Determines the importance of future rewards relative to immediate rewards. A high `γ` (close to 1) emphasizes long-term rewards, while a low `γ` prioritizes immediate rewards.
## Data Assumptions
`SARSA` makes similar assumptions to `Q-learning`:

- `Markov Property:` It assumes that the environment satisfies the Markov property, meaning that the next state depends only on the current state and action, not on the history of previous states and actions.
- `Stationary Environment:` It assumes that the environment's dynamics (transition probabilities and reward functions) do not change over time.