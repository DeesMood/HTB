The data component comprises everything related to the data the model operates on, including training data and inference data. Additionally, depending on the type of data a model operates on, a data leak can lead to legal consequences for the victim — for instance, GDPR-related consequences for data containing personally identifiable information (PII).
## Risks
The quality of an ML model depends on the quality of the data it was trained on. One of the most significant risks of systems using generative AI is improper training data. This can include biases in the training data and unrepresentative training data that does not match the kind of data the fully trained model is queried on. Issues in the training data set may lead to low-quality results generated by the fully trained model and discriminatory or harmful output. Therefore, creating proper training data is of utmost significance for the quality of the ML-based system.

`Data poisoning` is an attack vector with a similar impact to `model poisoning` discussed in the previous section. The main difference is that adversaries do not manipulate the model parameters directly but instead manipulate the training data during the training process. This type of data poisoning attack may have a similar impact to model poisoning, including:

- generation of misleading output
- generation of biased output
- generation of harmful content

In some cases, attackers may embed specific triggers in the data, leading the model to produce erroneous or adversarial outputs when prompted with specific inputs. This is known as a `backdoor attack`. Such manipulations can degrade the quality of the model, reduce trust in its outputs, or exploit it for malicious purposes like misinformation campaigns.

![](Pasted%20image%2020250825165330.png)
Due to the large amounts of data required to train and operate an ML model, there is an inherent risk of data leaks and unauthorized leakage of potentially sensitive data. Adversaries may exploit security vulnerabilities, resulting in unauthorized access to training data to obtain access to training data or inference data.

## Tactics, Techniques, and Procedures (TTPs)

As discussed above, the quality of any ML model is highly dependent on the `quality of the training data set`. As such, the consequences of adversaries manipulating training data in generative AI models extend beyond compromised performance. Introducing biased or false data could have severe ethical, legal, or safety implications in sensitive applications such as:

- content creation
- legal document generation
- AI-based healthcare advice

Therefore, training data manipulation is a lucrative attack vector for adversaries. However, an often challenging requirement for these attacks is knowing which data a model is trained on and injecting malicious data into the training data set. Depending on where the training data is fetched from, how it is sanitized or validated, and the exact setup of the training process, this requirement might be impossible to overcome. In other scenarios, manipulation of the training data or process is possible. For example, in `federated learning systems`, where multiple parties contribute to training, adversaries can inject poisoned updates during their participation, skewing the global model without raising suspicion.

Furthermore, adversaries interested in stealing training data from an ML-based system may use a mix of traditional and novel TTPs. These include identifying and exploiting weak security practices regarding data storage and transmission within the target organization. For instance:

- poorly configured cloud storage
- insufficient encryption at rest or at transit
- insecure data pipelines
- usage of vulnerable APIs

Additionally, adversaries may exploit the same vulnerabilities and misconfigurations in third-party vendors or data providers supplying or curating training data for generative AI models. Compromising a vendor in the ML model's supply chain allows attackers to access the dataset before it reaches the organization (`Supply Chain Attacks`).

Lastly, employees and contractors with legitimate access to sensitive data pose an insider threat. They may be exploited via traditional attack vectors such as phishing or social engineering to compromise credentials and obtain unauthorized access to data. On top of that, insider threats may deliberately exfiltrate training data for personal financial gain, industrial espionage, or other personal reasons. Since employees may have authorized access, they can steal data with little need for advanced hacking techniques or the presence of security vulnerabilities., making this threat harder to detect.




