[Ignore Previous Prompt: Attack Techniques For Language Models](https://arxiv.org/pdf/2211.09527)
[Effective Prompt Extraction from Language Models](https://arxiv.org/pdf/2307.06865)

https://arxiv.org/pdf/2302.12173 - Not what you’ve signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection

[GUARD:Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of LLMs](https://arxiv.org/pdf/2402.03299)

[Universal and Transferable Adversarial Attacks on Aligned Language Models](https://arxiv.org/pdf/2307.15043)

[“Do Anything Now”: Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models](https://arxiv.org/pdf/2308.03825)

[A Hitchhiker’s Guide to Jailbreaking ChatGPT via Prompt Engineering](https://dl.acm.org/doi/pdf/10.1145/3663530.3665021)

[Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks Warning: some content contains harmful language.](https://arxiv.org/pdf/2302.05733)



